{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA (Low Rank Adaptation) was  chosen due to its efficiency in fine-tuning large language models with fewer parameters, which helps mitigate computational costs while maintaining performance. It achieves this by introducing low-rank adaptations to the original model's attention mechanism, reducing the computational complexity and memory footprint.\n",
    "* Model: GPT-2ForSequenceClassification. This model architecture is selected for sentiment analysis, which aligns with the task at hand. GPT-2 is a well-established architecture known for its effectiveness in various NLP tasks, including classification. Additionally, by using GPT-2ForSequenceClassification, we leverage the pre-trained weights of GPT-2, which can capture rich linguistic patterns and contexts, potentially improving performance.\n",
    "* Evaluation approach: Evaluation before and after fine-tuning using the Trainer's `evaluate()` method. This approach provides a direct comparison of model performance before and after fine-tuning, ensuring the effectiveness of the fine-tuning process. By evaluating on the validation dataset using the same metrics and procedures, we can assess the impact of fine-tuning on model performance objectively.\n",
    "* Fine-tuning dataset: Stanford Sentiment Treebank - SST-2 because of the nature of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 35.3k/35.3k [00:00<00:00, 19.2MB/s]\n",
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 148k/148k [00:00<00:00, 750kB/s]\u001b[A\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:00,  4.75it/s]\n",
      "Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 19.1MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  5.23it/s]\n",
      "Downloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 796kB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  6.05it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1278.88it/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 325568.10 examples/s]\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 2039111.40 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 254624.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the Stanford Sentiment Treebank dataset\n",
    "# See: https://huggingface.co/datasets/sst2\n",
    "\n",
    "# Define the splits we want to load (training and testing)\n",
    "splits = [\"train\", \"validation\"]\n",
    "# Load the SST-2 dataset splits using a dictionary comprehension.\n",
    "# 'load_dataset' function fetches the dataset from Hugging Face's dataset repository.\n",
    "# 'glue' is the broader dataset collection, 'sst2' is the specific dataset for sentiment analysis.\n",
    "# Iterating over the splits list to load both training and testing sets.\n",
    "dataset = {split: load_dataset(\"glue\", \"sst2\", split=split) for split in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b63155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_subset(dataset, name_subset):\n",
    "    \"\"\"\n",
    "    A function for obtaining some statistics in a sunset of a HuggingFace dataset\n",
    "    :dataset: A Hugging Face dataset object\n",
    "    :name_subset: A string with the name of the subset\n",
    "    \n",
    "    :returns: No return\n",
    "    \"\"\"\n",
    "    # print number of samples in subset\n",
    "    print('Number of samples of', name_subset,'subset:',dataset[name_subset].num_rows)\n",
    "    # print maximum length of sequence in the subset\n",
    "    print('Max length of sentence in', name_subset, 'subset', max(len(sentence) for sentence in dataset[name_subset]['sentence']))\n",
    "    # print minimum length of sequence in the subset\n",
    "    print('Min length of sentence in', name_subset, 'subset', min(len(sentence) for sentence in dataset[name_subset]['sentence']))\n",
    "    # print labels in the subset\n",
    "    print('Labels in', name_subset,':', set(dataset[name_subset]['label']))\n",
    "    # print percentages of each label\n",
    "    print('Percentages for each label in subset:')\n",
    "    # compute frequencies for each label in the dataset\n",
    "    frequencies = {x: dataset[name_subset]['label'].count(x) for x in set(dataset[name_subset]['label'])}\n",
    "    # compute percentages\n",
    "    percentages = {x: (count / dataset[name_subset].num_rows) * 100 for x, count in frequencies.items()}\n",
    "    # loop over the keys in percentages and print values\n",
    "    for key, value in percentages.items():\n",
    "        print('- Label',key,':',round(value,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142dd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples of train subset: 67349\n",
      "Max length of sentence in train subset 268\n",
      "Min length of sentence in train subset 2\n",
      "Labels in train : {0, 1}\n",
      "Percentages for each label in subset:\n",
      "- Label 0 : 44.22 %\n",
      "- Label 1 : 55.78 %\n"
     ]
    }
   ],
   "source": [
    "# obtain statstics for train subset\n",
    "compute_statistics_subset(dataset=dataset, name_subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b48b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples of validation subset: 872\n",
      "Max length of sentence in validation subset 244\n",
      "Min length of sentence in validation subset 6\n",
      "Labels in validation : {0, 1}\n",
      "Percentages for each label in subset:\n",
      "- Label 0 : 49.08 %\n",
      "- Label 1 : 50.92 %\n"
     ]
    }
   ],
   "source": [
    "# obtain statstics for validation subset\n",
    "compute_statistics_subset(dataset=dataset, name_subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9f336",
   "metadata": {},
   "source": [
    "### Load Tokenizer and tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d661c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 55.3kB/s]\n",
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 1.65MB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.94MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 18.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 29.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4585a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set EOS (end of sentence) TOKEN as PAD TOKEN\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2f0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\n",
    "    :examples:\n",
    "    \n",
    "    :returns:\n",
    "    \"\"\"\n",
    "    # convert the text data in a list of tokens using tokenizer, truncating\n",
    "    # the text to the maximum lenght and pad shorter sequences to a uniform lenght\n",
    "    # return the result\n",
    "    return tokenizer(examples['sentence'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a66df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:06<00:00, 9935.15 examples/s] \n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 6920.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store the tokenized datasets.\n",
    "tokenized_ds = {}\n",
    "# Iterate over each data split ('train' and 'test').\n",
    "for split in splits:\n",
    "    # Apply the preprocess_function to the dataset corresponding to the current split.\n",
    "    # The 'map' function applies the preprocess_function to each example in the dataset.\n",
    "    # 'batched=True' allows processing multiple examples at once for efficiency.\n",
    "    tokenized_ds[split] = dataset[split].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e031a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hide new secretions from the parental units \n",
      "[24717, 649, 3200, 507, 422, 262, 21694, 4991, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "# print a sample of sentence and its tokenization in train subset\n",
    "print(tokenized_ds[\"train\"][0]['sentence'])\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7305f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's a charming and often affecting journey . \n",
      "[270, 705, 82, 257, 23332, 290, 1690, 13891, 7002, 764, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "# print a sample of sentence and its tokenization in validation subset\n",
    "print(tokenized_ds[\"validation\"][0]['sentence'])\n",
    "print(tokenized_ds[\"validation\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241a2c1",
   "metadata": {},
   "source": [
    "### Load model and freeze base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32a2208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 548M/548M [00:02<00:00, 220MB/s] \n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model 'gpt-2' for sequence classification.\n",
    "# This model is designed for tasks like sentiment analysis where each sequence (like a sentence)\n",
    "# is classified into categories (like positive/negative).\n",
    "# here, we specify the number of labels (2 for sentiment classification),\n",
    "# id2label and label2id corresponding to POSITIVE and NEGATIVE labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n",
    "                                                      num_labels=2,\n",
    "                                                      id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "                                                      label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adcd9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's pad token id to match the tokenizer's pad token id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7474d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters of the base model\n",
    "# Iterate over all the parameters of the base model.\n",
    "for param in model.base_model.parameters():\n",
    "    # freeze the base model disabling the gradient calculations for each parameter\n",
    "    # in the base model of \"gpt2\" model\n",
    "    # Set 'requires_grad' to False to freeze the parameters of the base model.\n",
    "    # Freezing prevents the weights of these layers from being updated during training.\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a29ad53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3735a6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 'model.score' is the classification head that will be trained to adapt \n",
    "# the base model for our specific task (sentiment analysis in this case).\n",
    "model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb1211b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Function for compute tha accuracy metric\n",
    "    :eval_pred: a tuple with predictions and labels\n",
    "    \n",
    "    :returns: a dictionary with the mean accuracy\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert the predictions to discrete labels by taking the argmax,\n",
    "    # which is the index of the highest value in the prediction (logits).\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Calculate and return the accuracy as the mean of the instances where\n",
    "    # predictions match the true labels.\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27644f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Initialize the Trainer, a high-level API for training transformer models.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\", # Directory where the model outputs will be saved.\n",
    "    learning_rate=2e-5, # Learning rate for the optimizer.\n",
    "    # Reduce the batch size if you don't have enough memory\n",
    "    per_device_train_batch_size=16, # Batch size for training per device.\n",
    "    per_device_eval_batch_size=16, # Batch size for evaluation per device.\n",
    "    num_train_epochs=1, # Number of training epochs.\n",
    "    weight_decay=0.01, # Weight decay for regularization.\n",
    "    evaluation_strategy=\"epoch\", # Evaluation is performed at the end of each epoch.\n",
    "    save_strategy=\"epoch\", # Model is saved at the end of each epoch.\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training.\n",
    ")\n",
    "\n",
    "pretrain_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"], # The tokenized training dataset.\n",
    "    eval_dataset=tokenized_ds[\"validation\"], # The tokenized evaluation dataset.\n",
    "    tokenizer=tokenizer, # The tokenizer used for encoding the data.\n",
    "    # Data collator that will dynamically pad the batches during training.\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics, # Function to compute metrics during evaluation.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d68bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results before fine-tuning: {'eval_loss': 1.800602912902832, 'eval_accuracy': 0.4908256880733945, 'eval_runtime': 3.7146, 'eval_samples_per_second': 234.751, 'eval_steps_per_second': 14.807}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set before fine-tuning\n",
    "pretrain_results = pretrain_trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results before fine-tuning\n",
    "print(\"Evaluation results before fine-tuning:\", pretrain_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ba434",
   "metadata": {},
   "source": [
    "In the cell output above, we can see that the model achieved about 0.49 evaluation accuracy, similar to flipping a coin. Let's fine-tune and see how we can improve this result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f320970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a2b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model 'gpt-2' for sequence classification.\n",
    "# This model is designed for tasks like sentiment analysis where each sequence (like a sentence)\n",
    "# is classified into categories (like positive/negative).\n",
    "# here, we specify the number of labels (2 for sentiment classification),\n",
    "# id2label and label2id corresponding to POSITIVE and NEGATIVE labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n",
    "                                                      num_labels=2,\n",
    "                                                      id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "                                                      label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a12ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's pad token id to match the tokenizer's pad token id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4580b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 814,080 || all params: 125,253,888 || trainable%: 0.6499438963523432\n"
     ]
    }
   ],
   "source": [
    "# Create a PEFT Config for LoRA\n",
    "config = LoraConfig(\n",
    "                    r=8, # Rank\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=['c_attn', 'c_proj'],\n",
    "                    lora_dropout=0.1,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_CLS\n",
    "                )\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edc85028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:00<00:00, 445606.09 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 120679.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Rename 'label' to 'labels' to match the Trainer's expectation\n",
    "tokenized_ds[\"train\"] = tokenized_ds[\"train\"].map(lambda e: {'labels': e['label']}, batched=True, remove_columns=['label'])\n",
    "tokenized_ds[\"validation\"] = tokenized_ds[\"validation\"].map(lambda e: {'labels': e['label']}, batched=True, remove_columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cb05ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds[\"train\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_ds[\"validation\"].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c53bae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,  # Make sure to pass the PEFT model here\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./lora_model_output\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='./logs',  # If you want to log metrics and/or losses during training\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True, max_length=512),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40467ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4210' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4210/4210 22:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.299321</td>\n",
       "      <td>0.879587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.290560</td>\n",
       "      <td>0.891055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4210, training_loss=0.47350006284736396, metrics={'train_runtime': 1361.6448, 'train_samples_per_second': 98.923, 'train_steps_per_second': 3.092, 'total_flos': 4437101443461120.0, 'train_loss': 0.47350006284736396, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6bf683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine tuned PEFT model\n",
    "peft_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f33779dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt-lora\", num_labels=NUM_LABELS, ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a331ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's pad token id to match the tokenizer's pad token id\n",
    "lora_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73bc6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Initialize the Trainer, a high-level API for training transformer models.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/sentiment_analysis\", # Directory where the model outputs will be saved.\n",
    "    learning_rate=2e-5, # Learning rate for the optimizer.\n",
    "    # Reduce the batch size if you don't have enough memory\n",
    "    per_device_train_batch_size=16, # Batch size for training per device.\n",
    "    per_device_eval_batch_size=16, # Batch size for evaluation per device.\n",
    "    num_train_epochs=1, # Number of training epochs.\n",
    "    weight_decay=0.01, # Weight decay for regularization.\n",
    "    evaluation_strategy=\"epoch\", # Evaluation is performed at the end of each epoch.\n",
    "    save_strategy=\"epoch\", # Model is saved at the end of each epoch.\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training.\n",
    ")\n",
    "\n",
    "finetuned_trainer = Trainer(\n",
    "    model=lora_model,  # The fine-tuned PEFT model.\n",
    "    args=training_args,# Training arguments, defined above.\n",
    "    train_dataset=tokenized_ds[\"train\"], # The tokenized training dataset.\n",
    "    eval_dataset=tokenized_ds[\"validation\"], # The tokenized evaluation dataset.\n",
    "    tokenizer=tokenizer, # The tokenizer used for encoding the data.\n",
    "    # Data collator that will dynamically pad the batches during training.\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics, # Function to compute metrics during evaluation.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd3a6354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the fine-tuned model: {'eval_loss': 0.29056042432785034, 'eval_accuracy': 0.8910550458715596, 'eval_runtime': 3.6751, 'eval_samples_per_second': 237.272, 'eval_steps_per_second': 14.966}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the validation set\n",
    "finetuned_results = finetuned_trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results for the fine-tuned model\n",
    "print(\"Evaluation results for the fine-tuned model:\", finetuned_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13432a95",
   "metadata": {},
   "source": [
    "In the cell output above, we can see that the model achieved an evaluation accuracy of about 0.89, a surprising result for 2 epochs of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac249f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
